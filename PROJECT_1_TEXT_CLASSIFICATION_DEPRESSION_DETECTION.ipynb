{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PROJECT_1_TEXT_CLASSIFICATION_DEPRESSION_DETECTION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/awannabecs/MY-FUNCTION-NAME/blob/main/PROJECT_1_TEXT_CLASSIFICATION_DEPRESSION_DETECTION.ipynb",
      "authorship_tag": "ABX9TyPsC0R7JWUoALtHzBSB0h49",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awannabecs/MY-FUNCTION-NAME/blob/main/PROJECT_1_TEXT_CLASSIFICATION_DEPRESSION_DETECTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f22i0WcfXOdv",
        "outputId": "c1d3d9da-2bfc-43be-ec60-f5c44d89f977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#import the libraries \n",
        "import numpy as  np\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "import nltk \n",
        "import re\n",
        "import matplotlib.pyplot as plt \n",
        "nltk.download(\"punkt\")\n",
        "tokenizer = nltk.word_tokenize\n",
        "tokenizer(\"go out\")\n",
        "stemmer = nltk.PorterStemmer().stem\n",
        "stemmer(\"played\")\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/MAIN_DATA\")\n",
        "df.dropna(inplace=False)"
      ],
      "metadata": {
        "id": "fxtuuIv5X611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a383152a-7117-41ee-d21c-ef3ad7b3c59b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0  Unnamed: 0.1  depressed  \\\n",
              "0              0           0.0        0.0   \n",
              "1              1           1.0        0.0   \n",
              "2              2           2.0        1.0   \n",
              "3              3           3.0        1.0   \n",
              "4              4           4.0        0.0   \n",
              "...          ...           ...        ...   \n",
              "1790         178        1210.0        1.0   \n",
              "1791         179        1212.0        1.0   \n",
              "1792         180        1233.0        1.0   \n",
              "1793         181        1234.0        1.0   \n",
              "1794         182        1250.0        1.0   \n",
              "\n",
              "                                                   text  \n",
              "0                 I know where you're coming from mate.  \n",
              "1                           I felt like this last year.  \n",
              "2     I remember around this time last year, i was h...  \n",
              "3     i was heavily addicted to meth, it was literal...  \n",
              "4     I worked like 70+hours in a warehouse job, 2 h...  \n",
              "...                                                 ...  \n",
              "1790                                  hate shattered me  \n",
              "1791   but I know depression and drug addiction don'...  \n",
              "1792   I feel a lot of deeply depressed souls have d...  \n",
              "1793            anxiety and mental breakdowns...|0|0|0|  \n",
              "1794                         learn to deal w my anxiety  \n",
              "\n",
              "[1790 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-007407cb-30b4-4f15-9bf0-06939b4898f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>depressed</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I know where you're coming from mate.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I felt like this last year.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>I remember around this time last year, i was h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>i was heavily addicted to meth, it was literal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I worked like 70+hours in a warehouse job, 2 h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1790</th>\n",
              "      <td>178</td>\n",
              "      <td>1210.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>hate shattered me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1791</th>\n",
              "      <td>179</td>\n",
              "      <td>1212.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>but I know depression and drug addiction don'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>180</td>\n",
              "      <td>1233.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>I feel a lot of deeply depressed souls have d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1793</th>\n",
              "      <td>181</td>\n",
              "      <td>1234.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>anxiety and mental breakdowns...|0|0|0|</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1794</th>\n",
              "      <td>182</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>learn to deal w my anxiety</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1790 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-007407cb-30b4-4f15-9bf0-06939b4898f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-007407cb-30b4-4f15-9bf0-06939b4898f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-007407cb-30b4-4f15-9bf0-06939b4898f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create the word2number dictionary\n",
        "tokens = []\n",
        "for i in range(len(df)):\n",
        "  words = []\n",
        "  try:\n",
        "    words.extend(tokenizer(df.iloc[i][\"text\"].lower()))\n",
        "    for word in words:\n",
        "      word = word\n",
        "      tokens.append(stemmer(word))\n",
        "  except Exception as e:\n",
        "    print(df.iloc[i][\"text\"])\n",
        "\n",
        "tokens = set(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbrKAVxQbBKk",
        "outputId": "21b7d02a-e87f-4549-acc8-3c5d7296757a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "somelist = [\" '//twitter.com/ltsharrypotter/status/984475138933579782',\", '//t.co/yacazn2t0h',\n",
        "            \"dk_mok\", '70+hour', 'much|1|0|4|', 'collection|1|0|3|',\"20\",]\n",
        "pattern = \"\"\"\\/?\\/?twitter.com/.+|\n",
        "\\/?\\/?t.co\\/?.+|  \n",
        "_|\n",
        "\\d+|\n",
        "\\|d\\|\\d\\|\\d|\n",
        "\"\"\"\n",
        "for word in somelist:\n",
        "  word = re.sub(pattern,\"\",word)\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrbpeNfqbZZn",
        "outputId": "dbfa45b1-20b1-455d-cf59-ce7a801384dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " '\n",
            "//t.co/yacazn2t0h\n",
            "dk_mok\n",
            "70+hour\n",
            "much|1|0|4|\n",
            "collection|1|0|3|\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token2index_dict = {\"OOV\":0}\n",
        "index2token_dict = {0:\"OOV\"}\n",
        "for index,token in enumerate(tokens):\n",
        "  token2index_dict[token] = index+1\n",
        "  index2token_dict[index+1] = token"
      ],
      "metadata": {
        "id": "3YagIXVaqgCo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token2index_function(dict,sent):\n",
        "  result_sent = []\n",
        "  for token in sent.split(\" \"):\n",
        "    result_sent.append(dict.get(token,0))\n",
        "  return result_sent\n",
        "\n",
        "\n",
        "def index2token_function(dict,index):\n",
        "  return dict.get(index,\"OOV\")"
      ],
      "metadata": {
        "id": "tf4ViBifw9lI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make the sentences same-lengthed \n",
        "#find the max length\n",
        "max_length = 0\n",
        "for sent in range(len(df)):\n",
        "  try:\n",
        "    words = nltk.word_tokenize(df.iloc[sent][\"text\"])\n",
        "  except Exception as e:\n",
        "    print(df.iloc[sent][\"text\"])\n",
        "  if len(words) > max_length:\n",
        "    max_length = len(words)"
      ],
      "metadata": {
        "id": "9GRKoCLy0W6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db36eb40-116f-42a9-a2a2-199ad0501d1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = [1,2,3,4,5,6,7,8]\n",
        "def same_lengther(vector,max_length):\n",
        "  zeros = np.zeros(max_length - len(vector))\n",
        "  vector.extend(zeros)\n",
        "  return vector"
      ],
      "metadata": {
        "id": "mTN-IxkN3KFc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define X and y\n",
        "X = []\n",
        "y = []\n",
        "X_vector = []\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  feature = df[\"text\"].iloc[i]\n",
        "  label = int(df[\"depressed\"].iloc[i])\n",
        "\n",
        "  try:\n",
        "    feature_vector = token2index_function(token2index_dict,feature)\n",
        "    X_vector.append(same_lengther(feature_vector,max_length))\n",
        "    y.append(label)\n",
        "    X.append(feature.lower())\n",
        "  except Exception as e:\n",
        "    print(i)\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRjE5Oy_8iGO",
        "outputId": "d1efc087-2956-46d3-ee15-ebe229db0f9d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "534\n",
            "727\n",
            "979\n",
            "1019\n",
            "1242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_vector = np.array(X_vector).astype(\"float32\")\n",
        "y = np.array(y).astype(\"float32\")\n",
        "\n",
        "X_vector.shape,y.shape\n",
        "y = pd.get_dummies(y)"
      ],
      "metadata": {
        "id": "Dxkx0WNI98gz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(666)\n",
        "model1 = tf.keras.Sequential()\n",
        "model1.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(64,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(32,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(16,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(8,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(4,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(2,activation=\"sigmoid\"))\n",
        "\n",
        "model1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "               optimizer=tf.keras.optimizers.Adam(),\n",
        "               metrics=[\"accuracy\"])\n",
        "\n",
        "model1.fit(X_vector,y,epochs=100)"
      ],
      "metadata": {
        "id": "7bq22Qhmy3Fm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1877ec4-7350-42f5-9006-4a7fadd6162d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "81/81 [==============================] - 3s 9ms/step - loss: 7.0201 - accuracy: 0.4784\n",
            "Epoch 2/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.7246 - accuracy: 0.4998\n",
            "Epoch 3/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.6947 - accuracy: 0.5010\n",
            "Epoch 4/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5018\n",
            "Epoch 5/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6924 - accuracy: 0.5025\n",
            "Epoch 6/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6924 - accuracy: 0.5025\n",
            "Epoch 7/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5025\n",
            "Epoch 8/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6924 - accuracy: 0.5025\n",
            "Epoch 9/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5025\n",
            "Epoch 10/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6923 - accuracy: 0.5025\n",
            "Epoch 11/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5021\n",
            "Epoch 12/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.6921 - accuracy: 0.5029\n",
            "Epoch 13/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5025\n",
            "Epoch 14/100\n",
            "81/81 [==============================] - 1s 6ms/step - loss: 0.6938 - accuracy: 0.5025\n",
            "Epoch 15/100\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.6921 - accuracy: 0.5025\n",
            "Epoch 16/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.4959\n",
            "Epoch 17/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5029\n",
            "Epoch 18/100\n",
            "81/81 [==============================] - 1s 6ms/step - loss: 0.6919 - accuracy: 0.5029\n",
            "Epoch 19/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6926 - accuracy: 0.5029\n",
            "Epoch 20/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5029\n",
            "Epoch 21/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5025\n",
            "Epoch 22/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6922 - accuracy: 0.5025\n",
            "Epoch 23/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5029\n",
            "Epoch 24/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5033\n",
            "Epoch 25/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6916 - accuracy: 0.5033\n",
            "Epoch 26/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6917 - accuracy: 0.5033\n",
            "Epoch 27/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5025\n",
            "Epoch 28/100\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.6921 - accuracy: 0.5033\n",
            "Epoch 29/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6917 - accuracy: 0.5033\n",
            "Epoch 30/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6916 - accuracy: 0.5033\n",
            "Epoch 31/100\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.6916 - accuracy: 0.5033\n",
            "Epoch 32/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6916 - accuracy: 0.4928\n",
            "Epoch 33/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6916 - accuracy: 0.5033\n",
            "Epoch 34/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6916 - accuracy: 0.5033\n",
            "Epoch 35/100\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.6920 - accuracy: 0.5060\n",
            "Epoch 36/100\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.6929 - accuracy: 0.5021\n",
            "Epoch 37/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.7015 - accuracy: 0.5006\n",
            "Epoch 38/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6951 - accuracy: 0.5014\n",
            "Epoch 39/100\n",
            "81/81 [==============================] - 1s 6ms/step - loss: 0.6930 - accuracy: 0.5014\n",
            "Epoch 40/100\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.6930 - accuracy: 0.5014\n",
            "Epoch 41/100\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.6930 - accuracy: 0.5014\n",
            "Epoch 42/100\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.4877\n",
            "Epoch 43/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.6929 - accuracy: 0.5014\n",
            "Epoch 44/100\n",
            "81/81 [==============================] - 1s 6ms/step - loss: 0.6929 - accuracy: 0.5014\n",
            "Epoch 45/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6929 - accuracy: 0.5014\n",
            "Epoch 46/100\n",
            "81/81 [==============================] - 1s 6ms/step - loss: 0.6929 - accuracy: 0.5014\n",
            "Epoch 47/100\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 0.6929 - accuracy: 0.5014\n",
            "Epoch 48/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5014\n",
            "Epoch 49/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5014\n",
            "Epoch 50/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5014\n",
            "Epoch 51/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.4943\n",
            "Epoch 52/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
            "Epoch 53/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 54/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 55/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4869\n",
            "Epoch 56/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4865\n",
            "Epoch 57/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 58/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 59/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 60/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 61/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 62/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 63/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 64/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 65/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4791\n",
            "Epoch 66/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 67/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 68/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4885\n",
            "Epoch 69/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 70/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 71/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4776\n",
            "Epoch 72/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4959\n",
            "Epoch 73/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5018\n",
            "Epoch 74/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 75/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 76/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 77/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4862\n",
            "Epoch 78/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4916\n",
            "Epoch 79/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 80/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4807\n",
            "Epoch 81/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4784\n",
            "Epoch 82/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 83/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 84/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 85/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 86/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 87/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 88/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 89/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 90/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 91/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4823\n",
            "Epoch 92/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 93/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4850\n",
            "Epoch 94/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4947\n",
            "Epoch 95/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 96/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4916\n",
            "Epoch 97/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4846\n",
            "Epoch 98/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "Epoch 99/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.4838\n",
            "Epoch 100/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b9125e7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.iloc[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BKhvfoVKFsc",
        "outputId": "4accccdf-6564-4855-dc71-c6424268e3c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    1\n",
              "1.0    0\n",
              "Name: 1, dtype: uint8"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# ***since we are getting aweful results, we geuess that our vectors are too sparse, so first we try to plot a histogram to see how many instances of different sizes we have.***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zlDXaNU0-iIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get a list of the length of the sentences \n",
        "len_sentences_list = []\n",
        "for i in range(len(df)):\n",
        "  try:\n",
        "    words = nltk.word_tokenize(df[\"text\"].iloc[i])\n",
        "    len_sentences_list.append(len(words))\n",
        "  except Exception as e:\n",
        "    print(df[\"text\"].iloc[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wFUUO1r-hRP",
        "outputId": "8084822e-b9c2-48dd-925b-5dfca4a76019"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the list as a histogram\n",
        "plt.hist(len_sentences_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KJwlgddabDpx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "d41d5c9a-eb4c-4133-8a3a-46f8cfd33fc5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# ***as you can see, we only have under 100 sentences with the length of 40,let alone the 121...so it's better to change the padding by setting the maximum length to be around 40 so we would not have such sparse vectors***\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NmBkz1zZAWJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1_X = []\n",
        "model1_y = []\n",
        "len_sentences_list = []\n",
        "for i in range(len(X)):\n",
        "  words = nltk.word_tokenize(X[i])\n",
        "  if len(words) <= 40:\n",
        "    model1_X.append(X[i])\n",
        "    len_sentences_list.append(len(words))\n",
        "    model1_y.append(y.iloc[i])"
      ],
      "metadata": {
        "id": "X44ccJiRAQGD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's replot the histogram to see what happened\n",
        "plt.hist(len_sentences_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "r8CNTkt4CFLa",
        "outputId": "0fa80380-cf33-4e85-8c83-3fd1a8da93d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# ***That looks better***\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FkniBA6LCyqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1_X_vectors = []\n",
        "for sentence in model1_X:\n",
        "  index_tokens = token2index_function(token2index_dict,sentence)\n",
        "  model1_X_vectors.append(np.array(same_lengther(index_tokens,40)))  \n",
        "model1_X_vectors = np.array(model1_X_vectors)\n",
        "model1_y = np.array(model1_y).astype(\"float32\")\n",
        "# model1_y = pd.get_dummies(model1_y)"
      ],
      "metadata": {
        "id": "iE7we3PJCj2L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1_X_vectors[0].shape,model1_y[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sWIR9RZD3rn",
        "outputId": "fa94be22-04ef-499c-842a-987cf90d6995"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((40,), (2,))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(666)\n",
        "model1 = tf.keras.Sequential()\n",
        "model1.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(64,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(32,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(16,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(8,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(4,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(2,activation=\"sigmoid\"))\n",
        "\n",
        "model1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "               optimizer=tf.keras.optimizers.Adam(),\n",
        "               metrics=[\"accuracy\"])\n",
        "\n",
        "model1.fit(model1_X_vectors,model1_y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xaV6AIeEuWn",
        "outputId": "432c1a95-7b45-481f-c52a-00b856862a76"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "79/79 [==============================] - 1s 3ms/step - loss: 9.4790 - accuracy: 0.4988\n",
            "Epoch 2/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.7280 - accuracy: 0.5004\n",
            "Epoch 3/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6960 - accuracy: 0.5040\n",
            "Epoch 4/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6977 - accuracy: 0.5048\n",
            "Epoch 5/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5067\n",
            "Epoch 6/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5091\n",
            "Epoch 7/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5087\n",
            "Epoch 8/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5083\n",
            "Epoch 9/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5067\n",
            "Epoch 10/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5087\n",
            "Epoch 11/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5083\n",
            "Epoch 12/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5083\n",
            "Epoch 13/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5071\n",
            "Epoch 14/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5091\n",
            "Epoch 15/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5051\n",
            "Epoch 16/100\n",
            "79/79 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5059\n",
            "Epoch 17/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6961 - accuracy: 0.5040\n",
            "Epoch 18/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5036\n",
            "Epoch 19/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5040\n",
            "Epoch 20/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 21/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 22/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 23/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 24/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 25/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 26/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 27/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 28/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 29/100\n",
            "79/79 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 30/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 31/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 32/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 33/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 34/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 35/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 36/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 37/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 38/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 39/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 40/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 41/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 42/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 43/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 44/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 45/100\n",
            "79/79 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 46/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 47/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 48/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 49/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 50/100\n",
            "79/79 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 51/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 52/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 53/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 54/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 55/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 56/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 57/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 58/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 59/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 60/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 61/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 62/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 63/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 64/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 65/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 66/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 67/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 68/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 69/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 70/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 71/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 72/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 73/100\n",
            "79/79 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 74/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 75/100\n",
            "79/79 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 76/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 77/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 78/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 79/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 80/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5040\n",
            "Epoch 81/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6999 - accuracy: 0.5036\n",
            "Epoch 82/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 83/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 84/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 85/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 86/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 87/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 88/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 89/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 90/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 91/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 92/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 93/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 94/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 95/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 96/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 97/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 98/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 99/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 100/100\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5036\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b910da590>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***As you can see,we still get unsatisfactory results, so we try to change our representation of the words. We are going to use another mothod that works with word probabilities.***"
      ],
      "metadata": {
        "id": "FGm62iQ2ASsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get the list of all words\n",
        "negative_sentences_words = []\n",
        "positive_sentences_words = []\n",
        "all_words = []\n",
        "for instance in range(len(df)):\n",
        "  itsclass = df.iloc[instance][\"depressed\"]\n",
        "  try:\n",
        "    if itsclass == 0:\n",
        "      negative_sentences_words.extend(tokenizer(df.iloc[instance][\"text\"].lower()))  \n",
        "    elif itsclass == 1:\n",
        "      positive_sentences_words.extend(tokenizer(df.iloc[instance][\"text\"].lower()))  \n",
        "  except Exception as e:\n",
        "    print(\"error\")\n",
        "\n",
        "\n",
        "all_words.extend(negative_sentences_words)\n",
        "all_words.extend(positive_sentences_words) \n",
        "all_words = set(all_words)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WclrGV-fAj0O",
        "outputId": "d0afef82-fc7f-4218-c9de-9c69cf0f2096"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_sentences_words_len = len(positive_sentences_words)\n",
        "negative_sentences_words_len = len(negative_sentences_words)\n",
        "positive_sentences_words_dict = Counter(positive_sentences_words)\n",
        "negative_sentences_words_dict = Counter(negative_sentences_words)"
      ],
      "metadata": {
        "id": "Q3bqF-fkCq95"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_sentences_words_len_unique = len(set(positive_sentences_words))\n",
        "negative_sentences_words_len_unique = len(set(negative_sentences_words))"
      ],
      "metadata": {
        "id": "EkCd4Y6X8iBo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GETTING THE LAMBDA \n",
        "all_words_lambdas = {}\n",
        "for word in all_words:\n",
        "  positive_word_freq = (positive_sentences_words_dict[word] + 1) / (positive_sentences_words_len + positive_sentences_words_len_unique)\n",
        "  negative_word_freq = (negative_sentences_words_dict[word] + 1) / (negative_sentences_words_len + negative_sentences_words_len_unique)\n",
        "  all_words_lambdas[word] = np.log(positive_word_freq / negative_word_freq)\n"
      ],
      "metadata": {
        "id": "V4IXXSkt-pw5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPqMM9QhsCL6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 42\n",
        "model3_X = []\n",
        "model3_y = []\n",
        "sentence_representations = []\n",
        "for index,sentence in enumerate(X):\n",
        "  sentence_representation = []\n",
        "  words = nltk.tokenize.word_tokenize(sentence)\n",
        "  words_len = len(words)\n",
        "  try:\n",
        "    padding_number = max_length - words_len\n",
        "    for word in words:\n",
        "      sentence_representation.append(all_words_lambdas[word])\n",
        "    sentence_representation.extend(np.zeros(padding_number))\n",
        "    model3_X.append(sentence_representation)\n",
        "    model3_y.append(df[\"depressed\"].iloc[index])\n",
        "  except Exception as e:\n",
        "    print(f\"length :{words_len}\")"
      ],
      "metadata": {
        "id": "BROM-o9zAm07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727a3a38-09a8-4e51-d3b6-8fd7e2e0b41f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length :43\n",
            "length :43\n",
            "length :45\n",
            "length :80\n",
            "length :70\n",
            "length :99\n",
            "length :86\n",
            "length :72\n",
            "length :68\n",
            "length :59\n",
            "length :45\n",
            "length :43\n",
            "length :54\n",
            "length :52\n",
            "length :48\n",
            "length :44\n",
            "length :43\n",
            "length :59\n",
            "length :121\n",
            "length :56\n",
            "length :53\n",
            "length :45\n",
            "length :51\n",
            "length :70\n",
            "length :43\n",
            "length :53\n",
            "length :56\n",
            "length :45\n",
            "length :47\n",
            "length :56\n",
            "length :45\n",
            "length :52\n",
            "length :62\n",
            "length :47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model3_X),len(model3_y)"
      ],
      "metadata": {
        "id": "xKhe6uHxCRj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d10129-2295-407d-e174-0d053d858a12"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2531, 2531)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3_y_ohe_narray = np.array(pd.get_dummies(model3_y))\n",
        "model3_X_narray = np.array(model3_X)\n"
      ],
      "metadata": {
        "id": "e2DYBzk1qk2E"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3_y_ohe_narray[1],model3_X_narray[1]\n",
        "print(len(model3_X_narray),len(model3_y_ohe_narray))\n",
        "print(model3_y_ohe_narray[1]),print(model3_X_narray[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVE5cnoytlMU",
        "outputId": "5db8963b-3da7-448c-f3eb-6a7a329fa46e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2531 2531\n",
            "[1 0]\n",
            "[ 1.06851488  2.6727896   0.99758843  0.11868242 -0.24865986 -0.18762397\n",
            "  0.43163883  0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.        ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(666)\n",
        "model1 = tf.keras.Sequential()\n",
        "model1.add(tf.keras.layers.Flatten())\n",
        "model1.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(64,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(32,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(16,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(8,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(4,activation=\"relu\"))\n",
        "model1.add(tf.keras.layers.Dense(2,activation=\"sigmoid\"))\n",
        "\n",
        "model1.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
        "               optimizer=tf.keras.optimizers.Adam(),\n",
        "               metrics=[\"accuracy\"])\n",
        "\n",
        "model1.fit(model3_X_narray,model3_y_ohe_narray,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSmli5f8s6xc",
        "outputId": "26bfa160-34a6-4cbd-918f-52f8542c2ed1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "80/80 [==============================] - 1s 3ms/step - loss: 0.1473 - accuracy: 0.7519\n",
            "Epoch 2/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1025 - accuracy: 0.8119\n",
            "Epoch 3/100\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0975 - accuracy: 0.8198\n",
            "Epoch 4/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.8234\n",
            "Epoch 5/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0929 - accuracy: 0.8317\n",
            "Epoch 6/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.8455\n",
            "Epoch 7/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.8514\n",
            "Epoch 8/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.8582\n",
            "Epoch 9/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.8605\n",
            "Epoch 10/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.8649\n",
            "Epoch 11/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.8716\n",
            "Epoch 12/100\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.8775\n",
            "Epoch 13/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.8775\n",
            "Epoch 14/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.8866\n",
            "Epoch 15/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.8795\n",
            "Epoch 16/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.8906\n",
            "Epoch 17/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.8949\n",
            "Epoch 18/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9016\n",
            "Epoch 19/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9052\n",
            "Epoch 20/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9048\n",
            "Epoch 21/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9024\n",
            "Epoch 22/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9064\n",
            "Epoch 23/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9115\n",
            "Epoch 24/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9119\n",
            "Epoch 25/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9115\n",
            "Epoch 26/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9147\n",
            "Epoch 27/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9162\n",
            "Epoch 28/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9143\n",
            "Epoch 29/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9253\n",
            "Epoch 30/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9253\n",
            "Epoch 31/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9257\n",
            "Epoch 32/100\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9131\n",
            "Epoch 33/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9226\n",
            "Epoch 34/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9261\n",
            "Epoch 35/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9277\n",
            "Epoch 36/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9324\n",
            "Epoch 37/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9261\n",
            "Epoch 38/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.9289\n",
            "Epoch 39/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.9332\n",
            "Epoch 40/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9320\n",
            "Epoch 41/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9376\n",
            "Epoch 42/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0394 - accuracy: 0.9344\n",
            "Epoch 43/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0379 - accuracy: 0.9344\n",
            "Epoch 44/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0364 - accuracy: 0.9419\n",
            "Epoch 45/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0395 - accuracy: 0.9328\n",
            "Epoch 46/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0377 - accuracy: 0.9364\n",
            "Epoch 47/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9316\n",
            "Epoch 48/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0365 - accuracy: 0.9368\n",
            "Epoch 49/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.9313\n",
            "Epoch 50/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.9324\n",
            "Epoch 51/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9388\n",
            "Epoch 52/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9427\n",
            "Epoch 53/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 0.9380\n",
            "Epoch 54/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0356 - accuracy: 0.9423\n",
            "Epoch 55/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0330 - accuracy: 0.9431\n",
            "Epoch 56/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.9423\n",
            "Epoch 57/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.9332\n",
            "Epoch 58/100\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9498\n",
            "Epoch 59/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.9372\n",
            "Epoch 60/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.9478\n",
            "Epoch 61/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.9482\n",
            "Epoch 62/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.9459\n",
            "Epoch 63/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.9490\n",
            "Epoch 64/100\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.9498\n",
            "Epoch 65/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.9510\n",
            "Epoch 66/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.9471\n",
            "Epoch 67/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.9510\n",
            "Epoch 68/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.9530\n",
            "Epoch 69/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.9526\n",
            "Epoch 70/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.9538\n",
            "Epoch 71/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.9475\n",
            "Epoch 72/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.9506\n",
            "Epoch 73/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9510\n",
            "Epoch 74/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.9542\n",
            "Epoch 75/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.9546\n",
            "Epoch 76/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.9510\n",
            "Epoch 77/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.9514\n",
            "Epoch 78/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.9502\n",
            "Epoch 79/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.9538\n",
            "Epoch 80/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.9589\n",
            "Epoch 81/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.9494\n",
            "Epoch 82/100\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0253 - accuracy: 0.9561\n",
            "Epoch 83/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0343 - accuracy: 0.9463\n",
            "Epoch 84/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.9471\n",
            "Epoch 85/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.9542\n",
            "Epoch 86/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9565\n",
            "Epoch 87/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.9585\n",
            "Epoch 88/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9550\n",
            "Epoch 89/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.9522\n",
            "Epoch 90/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9530\n",
            "Epoch 91/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9609\n",
            "Epoch 92/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9550\n",
            "Epoch 93/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9561\n",
            "Epoch 94/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.9561\n",
            "Epoch 95/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9577\n",
            "Epoch 96/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.9633\n",
            "Epoch 97/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9601\n",
            "Epoch 98/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9542\n",
            "Epoch 99/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.9561\n",
            "Epoch 100/100\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.9561\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b8bae6750>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The results are promising...but maybe TOO PROMISING....let's get out of our practice zone and get real by dividing the dataset into train,validation and test set to see if we are overfitting"
      ],
      "metadata": {
        "id": "78ZWNMunv8C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4_X = model3_X_narray\n",
        "model4_y = model3_y_ohe_narray\n",
        "trainset_index = int((len(model4_X) / 100) * 80)\n",
        "validationset_index = int((len(model4_X) - trainset_index ) / 2)\n",
        "trainset_X,trainset_y = model4_X[:trainset_index],model4_y[:trainset_index]\n",
        "validationset_X,validationset_y = model4_X[trainset_index:trainset_index+validationset_index],model4_y[trainset_index:trainset_index+validationset_index]\n",
        "testset_X,testset_y = model4_X[trainset_index+validationset_index:],model4_y[trainset_index+validationset_index:]"
      ],
      "metadata": {
        "id": "F_8bfhEktLn4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(trainset_X),len(trainset_y))\n",
        "print(len(validationset_X),len(validationset_y))\n",
        "print(len(testset_X),len(testset_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E9vs9m0yp2j",
        "outputId": "dbb06f39-01de-445a-f428-50d6a10e20c8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024 2024\n",
            "253 253\n",
            "254 254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(666)\n",
        "model4 = tf.keras.Sequential()\n",
        "model4.add(tf.keras.layers.Flatten())\n",
        "model4.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model4.add(tf.keras.layers.Dense(64,activation=\"relu\"))\n",
        "model4.add(tf.keras.layers.Dense(32,activation=\"relu\"))\n",
        "model4.add(tf.keras.layers.Dense(16,activation=\"relu\"))\n",
        "model4.add(tf.keras.layers.Dense(8,activation=\"relu\"))\n",
        "model4.add(tf.keras.layers.Dense(4,activation=\"relu\"))\n",
        "model4.add(tf.keras.layers.Dense(2,activation=\"sigmoid\"))\n",
        "\n",
        "model4.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
        "               optimizer=tf.keras.optimizers.Adam(),\n",
        "               metrics=[\"accuracy\"])\n",
        "model4.fit(trainset_X,trainset_y,validation_data=[validationset_X,validationset_y],epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aVrg9e0z9zj",
        "outputId": "c3253e78-a60c-48ed-932d-2f701c038999"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 1s 7ms/step - loss: 0.1570 - accuracy: 0.7273 - val_loss: 0.1316 - val_accuracy: 0.9842\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.7673 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.1209 - accuracy: 0.7757 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.7836 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.1147 - accuracy: 0.7905 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.1114 - accuracy: 0.7974 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.8073 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1053 - accuracy: 0.8241 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.8365 - val_loss: 0.0141 - val_accuracy: 0.9921\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.8414 - val_loss: 0.0051 - val_accuracy: 0.9960\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0939 - accuracy: 0.8498 - val_loss: 0.0056 - val_accuracy: 0.9921\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.8523 - val_loss: 0.0084 - val_accuracy: 0.9881\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 1s 8ms/step - loss: 0.0911 - accuracy: 0.8449 - val_loss: 0.0066 - val_accuracy: 0.9960\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 1s 8ms/step - loss: 0.0857 - accuracy: 0.8592 - val_loss: 0.0257 - val_accuracy: 0.9486\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0819 - accuracy: 0.8715 - val_loss: 0.0077 - val_accuracy: 0.9960\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 1s 8ms/step - loss: 0.0809 - accuracy: 0.8735 - val_loss: 0.0083 - val_accuracy: 0.9921\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 1s 8ms/step - loss: 0.0761 - accuracy: 0.8799 - val_loss: 0.0197 - val_accuracy: 0.9723\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.8814 - val_loss: 0.0702 - val_accuracy: 0.9130\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.8824 - val_loss: 0.0269 - val_accuracy: 0.9644\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 1s 14ms/step - loss: 0.0698 - accuracy: 0.8883 - val_loss: 0.0154 - val_accuracy: 0.9842\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.8948 - val_loss: 0.0126 - val_accuracy: 0.9723\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.8913 - val_loss: 0.0110 - val_accuracy: 0.9881\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.9012 - val_loss: 0.0064 - val_accuracy: 0.9921\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9007 - val_loss: 0.0295 - val_accuracy: 0.9605\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0643 - accuracy: 0.8982 - val_loss: 0.0173 - val_accuracy: 0.9842\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9007 - val_loss: 0.0134 - val_accuracy: 0.9881\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9017 - val_loss: 0.0085 - val_accuracy: 0.9881\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9012 - val_loss: 0.0082 - val_accuracy: 0.9921\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0557 - accuracy: 0.9091 - val_loss: 0.0111 - val_accuracy: 0.9881\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0573 - accuracy: 0.9086 - val_loss: 0.0166 - val_accuracy: 0.9684\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9096 - val_loss: 0.0103 - val_accuracy: 0.9881\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.9140 - val_loss: 0.0109 - val_accuracy: 0.9842\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0536 - accuracy: 0.9155 - val_loss: 0.0132 - val_accuracy: 0.9881\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.9200 - val_loss: 0.0139 - val_accuracy: 0.9842\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9165 - val_loss: 0.0191 - val_accuracy: 0.9802\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.9150 - val_loss: 0.0116 - val_accuracy: 0.9881\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0523 - accuracy: 0.9125 - val_loss: 0.0113 - val_accuracy: 0.9881\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9214 - val_loss: 0.0178 - val_accuracy: 0.9842\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0520 - accuracy: 0.9190 - val_loss: 0.0140 - val_accuracy: 0.9842\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9200 - val_loss: 0.0107 - val_accuracy: 0.9881\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0492 - accuracy: 0.9214 - val_loss: 0.0531 - val_accuracy: 0.9723\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9160 - val_loss: 0.0094 - val_accuracy: 0.9881\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0488 - accuracy: 0.9209 - val_loss: 0.0040 - val_accuracy: 0.9921\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0475 - accuracy: 0.9244 - val_loss: 0.0147 - val_accuracy: 0.9881\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0461 - accuracy: 0.9190 - val_loss: 0.0143 - val_accuracy: 0.9881\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0485 - accuracy: 0.9214 - val_loss: 0.0293 - val_accuracy: 0.9842\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0504 - accuracy: 0.9170 - val_loss: 0.0373 - val_accuracy: 0.9842\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 0.9298 - val_loss: 0.0134 - val_accuracy: 0.9881\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9249 - val_loss: 0.0100 - val_accuracy: 0.9881\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9229 - val_loss: 0.0368 - val_accuracy: 0.9881\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9269 - val_loss: 0.0022 - val_accuracy: 0.9921\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9254 - val_loss: 0.0051 - val_accuracy: 0.9921\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9274 - val_loss: 0.0117 - val_accuracy: 0.9881\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.9264 - val_loss: 0.0088 - val_accuracy: 0.9881\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.9343 - val_loss: 0.0172 - val_accuracy: 0.9842\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0418 - accuracy: 0.9318 - val_loss: 0.0168 - val_accuracy: 0.9881\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.9373 - val_loss: 0.0151 - val_accuracy: 0.9881\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0379 - accuracy: 0.9382 - val_loss: 0.0288 - val_accuracy: 0.9842\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9382 - val_loss: 0.0400 - val_accuracy: 0.9842\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9407 - val_loss: 0.0185 - val_accuracy: 0.9881\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.9387 - val_loss: 0.0493 - val_accuracy: 0.9802\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0389 - accuracy: 0.9328 - val_loss: 0.0387 - val_accuracy: 0.9842\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9363 - val_loss: 0.0479 - val_accuracy: 0.9802\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0372 - accuracy: 0.9402 - val_loss: 0.0548 - val_accuracy: 0.9802\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.9377 - val_loss: 0.0348 - val_accuracy: 0.9881\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9373 - val_loss: 0.0341 - val_accuracy: 0.9881\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.9377 - val_loss: 0.0424 - val_accuracy: 0.9842\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9323 - val_loss: 0.0343 - val_accuracy: 0.9881\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.0351 - accuracy: 0.9377 - val_loss: 0.0258 - val_accuracy: 0.9881\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9442 - val_loss: 0.0544 - val_accuracy: 0.9723\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9427 - val_loss: 0.0715 - val_accuracy: 0.9723\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.9422 - val_loss: 0.0406 - val_accuracy: 0.9881\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.9358 - val_loss: 0.0536 - val_accuracy: 0.9842\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.9427 - val_loss: 0.0777 - val_accuracy: 0.9723\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.9432 - val_loss: 0.0434 - val_accuracy: 0.9881\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.9466 - val_loss: 0.0415 - val_accuracy: 0.9881\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.9457 - val_loss: 0.0524 - val_accuracy: 0.9842\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.9417 - val_loss: 0.0342 - val_accuracy: 0.9881\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0348 - accuracy: 0.9432 - val_loss: 0.1176 - val_accuracy: 0.9565\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9452 - val_loss: 0.0444 - val_accuracy: 0.9881\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.9422 - val_loss: 0.0469 - val_accuracy: 0.9881\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.9471 - val_loss: 0.0362 - val_accuracy: 0.9881\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.9348 - val_loss: 0.0491 - val_accuracy: 0.9802\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9452 - val_loss: 0.0632 - val_accuracy: 0.9842\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.9476 - val_loss: 0.0767 - val_accuracy: 0.9842\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.9402 - val_loss: 0.0562 - val_accuracy: 0.9842\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.9392 - val_loss: 0.0664 - val_accuracy: 0.9842\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.9471 - val_loss: 0.0553 - val_accuracy: 0.9842\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9457 - val_loss: 0.0462 - val_accuracy: 0.9842\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9447 - val_loss: 0.0378 - val_accuracy: 0.9881\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9353 - val_loss: 0.0683 - val_accuracy: 0.9723\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9363 - val_loss: 0.0524 - val_accuracy: 0.9842\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9501 - val_loss: 0.0636 - val_accuracy: 0.9763\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.9471 - val_loss: 0.0813 - val_accuracy: 0.9763\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9471 - val_loss: 0.1094 - val_accuracy: 0.9723\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9486 - val_loss: 0.0404 - val_accuracy: 0.9881\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9501 - val_loss: 0.0923 - val_accuracy: 0.9684\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.9501 - val_loss: 0.1198 - val_accuracy: 0.9644\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.9550 - val_loss: 0.0646 - val_accuracy: 0.9763\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9432 - val_loss: 0.1120 - val_accuracy: 0.9723\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b9114b2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_testset_y = model4.predict(testset_X)\n",
        "print(sum(predicted_testset_y == testset_y)[0]/ len(testset_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6MPrHxW1z84",
        "outputId": "cd8a6436-c6f7-4086-9dd8-5686a5a1bae5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Wait...what happened???High train accuracy set and high validation set but low test set***\n",
        "\n",
        "# **1)shuffling dataset**\n",
        "# **2)using cross validation**\n",
        "# **3)see whether the classes are balanced**"
      ],
      "metadata": {
        "id": "yXDzeH_dBmgL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SO7aMtlVCrW_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}